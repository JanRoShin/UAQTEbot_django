# -*- coding: utf-8 -*-
"""w/o_attention

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Syvz6G9r0CqTws1wKIc7rTJqCvBCzisk


from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/UAQTEbot/
"""
import pandas as pd
import string
import nltk
import re
import random
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, Dropout, Attention, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split, GridSearchCV
from nltk.translate.bleu_score import sentence_bleu

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Read the CSV file into a DataFrame
df = pd.read_csv("UAQTEbot_dataset1.csv")

# Function to remove rows with missing values
def remove_rows_with_missing_values(df, columns):
    missing_values_mask = df[columns].isnull().any(axis=1)
    cleaned_df = df[~missing_values_mask]
    return cleaned_df

df = remove_rows_with_missing_values(df, ['Questions', 'Answers'])

ques = df['Questions'].tolist()
ans = df['Answers'].tolist()

# Text Preprocessing
def preprocess_text(text):
    # Remove punctuation
    text = re.sub(r"[^\w\s]", "", text)

    # Reformat Contractions
    contraction_mapping = {
        "arent": ["are not"],
        "cant": ["cannot"],
        "couldnt": ["could not"],
        "didnt": ["did not"],
        "doesnt": ["does not"],
        "dont": ["do not"],
        "hadnt": ["had not"],
        "hasnt": ["has not"],
        "havent": ["have not"],
        "hed": ["he had", "he would"],
        "hell": ["he will", "he shall"],
        "hes": ["he is", "he has"],
        "id": ["i had", "i would"],
        "ill": ["i will", "i shall"],
        "im": ["i am"],
        "ive": ["i have"],
        "isnt": ["is not"],
        "its": ["it is", "it has"],
        "lets": ["let us"],
        "mustnt": ["must not"],
        "shant": ["shall not"],
        "shed": ["she had", "she would"],
        "shell": ["she will", "she shall"],
        "shes": ["she is", "she has"],
        "shouldnt": ["should not"],
        "thats": ["that is", "that has"],
        "theres": ["there is", "there has"],
        "theyd": ["they had", "they would"],
        "theyll": ["they will", "they shall"],
        "theyre": ["they are"],
        "theyve": ["they have"],
        "wed": ["we had", "we would"],
        "were": ["we are"],
        "weve": ["we have"],
        "werent": ["were not"],
        "whatll": ["what will", "what shall"],
        "whatre": ["what are"],
        "whats": ["what is", "what has"],
        "whatve": ["what have"],
        "wheres": ["where is", "where has"],
        "whod": ["who had", "who would"],
        "wholl": ["who will", "who shall"],
        "whore": ["who are"],
        "whos": ["who is", "who has"],
        "whove": ["who have"],
        "wont": ["will not"],
        "wouldnt": ["would not"],
        "youd": ["you had", "you would"],
        "youll": ["you will", "you shall"],
        "youre": ["you are"],
        "youve": ["you have"]
    }

    def replace_contractions(match):
        contraction = match.group(0).lower()
        if contraction in contraction_mapping:
            options = contraction_mapping[contraction]
            return random.choice(options)
        else:
            return match.group(0)

    text = re.sub(r"\b(?:{})\b".format("|".join(contraction_mapping.keys())), replace_contractions, text, flags=re.IGNORECASE)

    return text

# Apply text preprocessing
cl_ques = [preprocess_text(Questions.lower()) for Questions in ques]
cl_ans = [preprocess_text(Answers.lower()) for Answers in ans]

# Define vocabulary
vocab = {'<PAD>': 0, '<EOS>': 1, '<OUT>': 2, '<SOS>': 3}
word_num = len(vocab)
for line in cl_ques + cl_ans:
    for word in line.split():
        if word not in vocab:
            vocab[word] = word_num
            word_num += 1

# Add special tokens to vocabulary
vocab['cameron'] = vocab['<PAD>']
vocab['<PAD>'] = 0


# Train Word2Vec model
sentences = [text.split() for text in cl_ques + cl_ans]
word2vec_model = Word2Vec(sentences, vector_size=1024, window=20, min_count=4, workers=16)

# Get Word2Vec embeddings and vocabulary
word_vectors = word2vec_model.wv
word_vocab = word_vectors.key_to_index

# Replace the existing embedding layer with Word2Vec embeddings
embedding_matrix = np.zeros((len(vocab), word_vectors.vector_size))
for word, i in vocab.items():
    if word in word_vocab:
        embedding_vector = word_vectors[word]
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

# Create a custom embedding layer using Word2Vec embeddings
embedding_layer = Embedding(len(vocab), word_vectors.vector_size, trainable=False)
embedding_layer.build((None,))
embedding_layer.set_weights([embedding_matrix])

# Tokenize and pad sequences
encoder_inp = pad_sequences([[vocab.get(word, vocab['<OUT>']) for word in line.split()] for line in cl_ques], padding='post')
decoder_inp = pad_sequences([[vocab.get(word, vocab['<OUT>']) for word in line.split()] for line in cl_ans], maxlen=516, padding='post')
decoder_final_output = to_categorical(pad_sequences([[vocab.get(word, vocab['<OUT>']) for word in line.split()[1:]] for line in cl_ans], padding='post'), len(vocab))

# Define model architecture
enc_inp = Input(shape=(encoder_inp.shape[1],))
dec_inp = Input(shape=(decoder_inp.shape[1],))

enc_embed = embedding_layer(enc_inp)
enc_lstm = LSTM(700, return_sequences=True, return_state=True)
enc_op, enc_state_h, enc_state_c = enc_lstm(enc_embed)
enc_states = [enc_state_h, enc_state_c]

dec_embed = embedding_layer(dec_inp)
dec_lstm = LSTM(700, return_sequences=True, return_state=True)
dec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)
dense = Dense(len(vocab), activation='softmax')
dense_op = dense(dec_op)

# Compile and train the model
model = Model([enc_inp, dec_inp], dense_op)
model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')

# Train the model
model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs=1000)

# Evaluate model performance on validation set
loss, accuracy = model.evaluate([encoder_inp, decoder_inp], decoder_final_output)
print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')

# Save the trained model
model.save('my_model.keras')

# Define encoder and decoder models
enc_model = Model(enc_inp, enc_states)
decoder_state_input_h = Input(shape=(700,))
decoder_state_input_c = Input(shape=(700,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs, state_h, state_c = dec_lstm(dec_embed, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
dec_model = Model([dec_inp] + decoder_states_inputs, [decoder_outputs] + decoder_states)


def calculate_bleu(reference, candidate):
    reference = [reference.split()]
    candidate = candidate.split()
    return sentence_bleu(reference, candidate)

def calculate_bleu(reference, candidate):
    reference = [reference.split()]
    candidate = candidate.split()
    return sentence_bleu(reference, candidate)

print("Start chatting:")
total_bleu_score = 0.0
num_samples = len(cl_ans)  # Assuming cl_ans contains ground truth answers
for i in range(num_samples):
    user_input = input("You: ")
    if user_input.lower() == 'exit':
        break
    user_input = preprocess_text(user_input.lower())
    input_seq = pad_sequences([[vocab.get(word, vocab['<OUT>']) for word in user_input.split()]], encoder_inp.shape[1], padding='post')
    states_value = enc_model.predict(input_seq)
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = vocab['<SOS>']
    stop_condition = False
    decoded_translation = ''
    while not stop_condition:
        output_tokens, h, c = dec_model.predict([target_seq] + states_value)
        sampled_word_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = [word for word, index in vocab.items() if index == sampled_word_index][0]
        if sampled_word != '<EOS>':
            decoded_translation += sampled_word + ' '
        if sampled_word == '<EOS>' or len(decoded_translation.split()) > 100:
            stop_condition = True
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_word_index
        states_value = [h, c]
    print("Chatbot: ", decoded_translation)

    # Calculate BLEU score for the current response
    bleu_score = calculate_bleu(cl_ans[i], decoded_translation)
    print("BLEU Score:", bleu_score)
    total_bleu_score += bleu_score

# Calculate average BLEU score over all samples
average_bleu_score = total_bleu_score / num_samples
print("Average BLEU Score:", average_bleu_score)